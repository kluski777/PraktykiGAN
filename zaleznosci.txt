Proba, Modyfikacja, wynik, decyzja
1, Zmniejszenie beta0 na Adamie D do 0.5 beta1=0.999, bez roznicy rozklad jest ladniejszy ale nadal generator dominuje, zostaje
2, Zmniejszenie beta1 na Adamie na D do 0.9, dominacja G rozklad brzydszy, powrot
3, ustawienie alfy na RMSie na G do 0.9, szybka dominacja G, powrot
4, Zmniejszenie liczby zwracanych czastek przez G z 1024 -> 512, szybciej leci do nieskonczonosci, zostaje
5, Liczba warstw G 4 -> 2, wolno ucieka do nieskonczonosci G slabszy, zostaje
6, NOISE_SIZE 128 -> 512, jeszcze szybciej dominuje, powrot
7, NOISE_SIZE 128 -> 32, dominuje w ok. 800 epoce, powrot 
8, funkcja straty na D MSE -> BCE, dominacja G od 700 epoki, powrot
9, funkcja straty na G MSE -> BCE, dominacja G od 900 epoki, zostaje
10, optim na G RMSprop -> SGD, dominacja G od 800 epoki, powrot
11, usuniecie ciecia gradientow na G, dominacja G od 700 epoki, powrot
12, ciecie grad norm na G do 0.5, dominacja G od 700 epoki, powrot
13, D optim ADAM -> RMSprop, diminacja G od 400 epoki, powrot
14, beta0 w ADAMie na D 0.5 -> 0.9, dominacja G od 700 epoki, powrot
15, LR_RATIO 2 -> 10, od 150 epoki dominacja G, powrot
16, Rozszerzenie D w srodku rozklad neuronow [GENERATOR_SAMPLES_TO_RETURN, 1024, 1296, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1], silniejszy D brak dominacji G ale wyniki G ida do nieskonczonosci powolne zmiany wag, zostaje
17, Zwezenie G neuron_numbers = [NOISE_SIZE, 16, 64, 128, GENERATOR_SAMPLES_TO_RETURN] zwiekszenie liczby warstw 2 -> 6, brak dominacji ale x i y ida do inf, powrot
18, Mocniejszy bottleneck na G neuron_numbers = [NOISE_SIZE, 4, 16, 128, GENERATOR_SAMPLES_TO_RETURN], brak dominacji x i y leca do inf ale wolniej, zostaje
19, MSE -> BCE na D, Dominacja G od 300 epoki, powrot
20, na D tanh w ostatniej warstwie, Dominacja G od 1000 epoki, powrot
21, na D sigmoid w ostatniej warstwie, brak dominacji mniejsze zmiany wag na D potencjalna stabilnosc, zostaje
22, LR_RATIO 2 -> 5, D wyrabia nad G ale uczenie nieco niestabilne, zostaje  
23, LR_RATIO 5 -> 15 LR_CONSTANT 1e-3 -> 5e-4, obydwa sie ucza probki bardzo szybko ida do nieskonczonosci, zostaje
24, zobaczyc prawdziwe dane, prawdziwe dane sa ok wartosc maksymalna to 75 tymaczasem generator ucieka do nieskonczonosci i dyskryminator tego nie widzdi
25, batchNorm1d na generator, duzo wolniej rosna x_fake i y_fake, powrot
26, podzielenie prawdziwych wartosci przez r_max, dominacja G x i tak dazy do duzych wartosci, powrot
27, usuniecie batchnorma z dyskryminatora, dominacja G od samego poczatku, zostaje bo mam nadzieje ze to powodowalo uciekanie x_true do inf
28, zmniejszenie LR_CONSTANT 5e-4 -> 1e-4, rozklad ladny ale generator dominuje, zostaje
29, Hinge_Loss na D i G, szybka dominacja G, powrot
30, Hinge_Loss ale tylko na G i RMSprop na D, 
