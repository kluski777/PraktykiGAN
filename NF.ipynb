{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "15c6f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nf.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from walidacja_funkcji import *\n",
    "\n",
    "prober = rs_prober_NKG(epsilon=0.1, looking_x_left=-1, looking_x_right=1, from_x=0)\n",
    "\n",
    "def get_2d_data(PROBE_SIZE: int):\n",
    "    rs = prober.rejection_sampling(length=PROBE_SIZE // 2)\n",
    "    theta = np.random.uniform(0, 2*np.pi, size=PROBE_SIZE // 2)\n",
    "    xs = rs * np.cos(theta)\n",
    "    ys = rs * np.sin(theta)\n",
    "    return np.vstack([xs, ys]).T\n",
    "\n",
    "\n",
    "class NF_layer(nn.Module):\n",
    "    def __init__(self, translate_layers: list[nn.Module], scale_layers: list[nn.Module], lr):\n",
    "        super().__init__()\n",
    "        self.translate = nn.Sequential(*translate_layers)\n",
    "        self.scale = nn.Sequential(*scale_layers)\n",
    "        self.optim = torch.optim.RMSprop(self.parameters(), lr=lr) # tu nie jestem pewny moze dwa oddzielne optimizery powinny isc idk\n",
    "\n",
    "        self.to('cuda')\n",
    "\n",
    "    # mozna dodac loss_backward z funkcja probkowania z NKG\n",
    "    def loss(self, output: torch.Tensor, log_diag: torch.Tensor = None): \n",
    "        return 0.5 * (output ** 2).mean() - log_diag.mean() # No czekaj ale do czego to tak naprawde zmierza\n",
    "\n",
    "    def loss_and_step(self, output: torch.Tensor, log_diag: torch.Tensor = None):\n",
    "        self.zero_grad()\n",
    "        loss = self.loss(output, log_diag)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def calculate_forward(self, input: torch.Tensor, function: callable) -> torch.Tensor:\n",
    "        div_indx = input.shape[-1] // 2\n",
    "        x1 = input[:, :div_indx]\n",
    "        x2 = input[:, div_indx:]\n",
    "\n",
    "        scaled = self.scale(x1)\n",
    "        translated = self.translate(x1)\n",
    "        diag_sum = scaled # we wzorze jest dzielenie przez N, czyli srednia troche nawet zawyza wynik, ale to chyba dobrze\n",
    "        x1 = function(x2, scaled, translated)\n",
    "        z = torch.cat([x2, x1], dim=-1) # swap\n",
    "\n",
    "        return z, diag_sum, x1\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return self.calculate_forward(input, lambda x2, scaled, translated: (x2 + translated) * torch.exp(scaled))\n",
    "\n",
    "    def inverse(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return self.calculate_forward(input, lambda x2, scaled, translated: x2 / torch.exp(scaled) - translated)\n",
    "\n",
    "\n",
    "class NF(nn.Module):\n",
    "    def __init__(self, num_layers: int, input_size: int, lr: float):\n",
    "        super().__init__()\n",
    "\n",
    "        def scaling_components():\n",
    "            return [\n",
    "                nn.Linear(input_size, input_size), nn.Tanh(),\n",
    "                nn.Linear(input_size, input_size), \n",
    "                nn.Linear(input_size, input_size), nn.Tanh(),\n",
    "                nn.Linear(input_size, input_size), \n",
    "                nn.Linear(input_size, input_size), \n",
    "            ]\n",
    "\n",
    "        def translating_components():\n",
    "            return [\n",
    "                nn.Linear(input_size, input_size), \n",
    "                nn.Linear(input_size, input_size), \n",
    "            ]\n",
    "\n",
    "        self.layers = nn.ModuleList([NF_layer(translating_components(), scaling_components(), lr) for _ in range(num_layers)])\n",
    "        self.to('cuda')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.iterate_layers(x, forward=True, learn=False)\n",
    "\n",
    "    def inverse(self, x):\n",
    "        return self.iterate_layers(x, forward=False, learn=False)\n",
    "\n",
    "    def loss_and_step(self, x):\n",
    "        return self.iterate_layers(x, forward=True, learn=True)\n",
    "\n",
    "    def iterate_layers(self, input: torch.Tensor, forward: bool, learn: bool):\n",
    "        output = input.clone()\n",
    "        transformed = None\n",
    "        losses = 0\n",
    "\n",
    "        iter_layers = self.layers if forward else reversed(self.layers)\n",
    "        for layer in iter_layers:\n",
    "            if forward:\n",
    "                output, diag_sum, transformed = layer.forward(output)\n",
    "            else:\n",
    "                output, diag_sum, transformed = layer.inverse(output)\n",
    "\n",
    "            if learn:\n",
    "                losses += layer.loss_and_step(transformed, diag_sum)\n",
    "                output = output.detach()\n",
    "\n",
    "        if learn:\n",
    "            return losses\n",
    "\n",
    "        return output\n",
    "\n",
    "def learn_nf(nf_model):\n",
    "    values = get_2d_data(PROBE_SIZE=PROBE_SIZE).reshape(BATCH_SIZE, -1, 2)\n",
    "    x = torch.Tensor(values[:, :, 0]).cuda()\n",
    "    loss_history = np.empty(EPOCHS)\n",
    "    \n",
    "    nf_model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\r{epoch / (EPOCHS - 1) * 100:.1f}%', end='', flush=True)\n",
    "        loss_history[epoch] = nf_model.loss_and_step(x)\n",
    "        if epoch % 5 == 0:\n",
    "            values = get_2d_data(PROBE_SIZE=PROBE_SIZE).reshape(BATCH_SIZE, -1, 2)\n",
    "            x = torch.Tensor(values[:, :, 0]).cuda()\n",
    "    \n",
    "    nf_model.eval()\n",
    "    \n",
    "    plt.plot(np.log10(loss_history), 'o', markersize=0.3)\n",
    "    plt.title(r'$log_{10}$(Loss)')\n",
    "    plt.show()\n",
    "    \n",
    "    temp = get_2d_data(PROBE_SIZE=PROBE_SIZE * 10).reshape(BATCH_SIZE * 10, -1, 2)\n",
    "    x = torch.Tensor(temp[:, :, 0]).cuda()\n",
    "    theory = np.random.randn(PROBE_SIZE * 5)\n",
    "    values = nf_model.forward(x)\n",
    "    inversed = nf_model.inverse(values).detach().flatten().cpu().numpy()\n",
    "    values = values.detach().flatten().cpu().numpy()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "    \n",
    "    ax1.hist(inversed, bins=100, range=[-5, 5], label='f⁻¹(f(x))', \n",
    "            color=colors[0], alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    ax1.hist(x.detach().flatten().cpu().numpy(), bins=100, range=[-5, 5], label='Original x',\n",
    "            color=colors[1], alpha=0.6, edgecolor='white', linewidth=0.5)\n",
    "    ax1.set_title('Invertibility Check', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Value', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.legend(frameon=False, fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.hist(values, bins=100, range=[-5, 5], label='After NF',\n",
    "            color='#FF1744', alpha=0.8, edgecolor='white', linewidth=0.5)\n",
    "    ax2.hist(theory, bins=100, range=[-5, 5], label='Normal Distribution',\n",
    "            color='#00E676', alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    ax2.set_title('Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Value', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.legend(frameon=False, fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
