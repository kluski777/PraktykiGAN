Proba, Modyfikacja, wynik, decyzja
1, Zmniejszenie beta0 na Adamie D do 0.5 beta1=0.999, bez roznicy rozklad jest ladniejszy ale nadal generator dominuje, zostaje
2, Zmniejszenie beta1 na Adamie na D do 0.9, dominacja G rozklad brzydszy, powrot
3, ustawienie alfy na RMSie na G do 0.9, szybka dominacja G, powrot
4, Zmniejszenie liczby zwracanych czastek przez G z 1024 -> 512, szybciej leci do nieskonczonosci, zostaje
5, Liczba warstw G 4 -> 2, wolno ucieka do nieskonczonosci G slabszy, zostaje
6, NOISE_SIZE 128 -> 512, jeszcze szybciej dominuje, powrot
7, NOISE_SIZE 128 -> 32, dominuje w ok. 800 epoce, powrot 
8, funkcja straty na D MSE -> BCE, dominacja G od 700 epoki, powrot
9, funkcja straty na G MSE -> BCE, dominacja G od 900 epoki, zostaje
10, optim na G RMSprop -> SGD, dominacja G od 800 epoki, powrot
11, usuniecie ciecia gradientow na G, dominacja G od 700 epoki, powrot
12, ciecie grad norm na G do 0.5, dominacja G od 700 epoki, powrot
13, D optim ADAM -> RMSprop, diminacja G od 400 epoki, powrot
14, beta0 w ADAMie na D 0.5 -> 0.9, dominacja G od 700 epoki, powrot
15, LR_RATIO 2 -> 10, od 150 epoki dominacja G, powrot
16, Rozszerzenie D w srodku rozklad neuronow [GENERATOR_SAMPLES_TO_RETURN, 1024, 1296, 1024, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1], silniejszy D brak dominacji G ale wyniki G ida do nieskonczonosci powolne zmiany wag, zostaje
17, Zwezenie G neuron_numbers = [NOISE_SIZE, 16, 64, 128, GENERATOR_SAMPLES_TO_RETURN] zwiekszenie liczby warstw 2 -> 6, brak dominacji ale x i y ida do inf, powrot
18, Mocniejszy bottleneck na G neuron_numbers = [NOISE_SIZE, 4, 16, 128, GENERATOR_SAMPLES_TO_RETURN], brak dominacji x i y leca do inf ale wolniej, zostaje
19, MSE -> BCE na D, Dominacja G od 300 epoki, powrot
20, na D tanh w ostatniej warstwie, Dominacja G od 1000 epoki, powrot
21, na D sigmoid w ostatniej warstwie, brak dominacji mniejsze zmiany wag na D potencjalna stabilnosc, zostaje
22, LR_RATIO 2 -> 5, D wyrabia nad G ale uczenie nieco niestabilne, zostaje  
23, LR_RATIO 5 -> 15 LR_CONSTANT 1e-3 -> 5e-4, obydwa sie ucza probki bardzo szybko ida do nieskonczonosci, zostaje
24, zobaczyc prawdziwe dane, prawdziwe dane sa ok wartosc maksymalna to 75 tymaczasem generator ucieka do nieskonczonosci i dyskryminator tego nie widzdi
25, batchNorm1d na generator, duzo wolniej rosna x_fake i y_fake, powrot
26, podzielenie prawdziwych wartosci przez r_max, dominacja G x i tak dazy do duzych wartosci, powrot
27, usuniecie batchnorma z dyskryminatora, dominacja G od samego poczatku, zostaje bo mam nadzieje ze to powodowalo uciekanie x_true do inf
28, zmniejszenie LR_CONSTANT 5e-4 -> 1e-4, rozklad ladny ale generator dominuje, zostaje
29, Hinge_Loss na D i G, szybka dominacja G, powrot
30, LR_CONSTANT 1e-4 -> 5e-5 LR_RATIO 15 -> 30, rownowaga - brak dominacji ale powolne uczenie, zostaje
31, LR_CONSTANT 5e-5 -> 2e-4, calkowita dominacja G, powrot
32, dalsze rozszerzenie D, dyskryminator dominuje ale jest nagla eksplozja wagi, zostaje
33, zmniejszenie bottlenecka w G, dyskryminator przegrywa do momentu kiedy nastepuje *cos* i strata dyskryminatora skokowo rosnie po czym ten ciagle przegrywa, zostaje
34, dalsze rozszerzenie G, za wolne uczenie, powrot
35, szukanie odpowiedniej szerokosci G, wybuch gradientu na dyskryminatorze, zostaje
36, obcinanie gradientow do grad_norm = 1.0 tylko na G zeby D nadrobil, stabilne uczenie, powrot
37, powrot do 30, ale o dziwo dyskryminator dominuje G sie nie uczy, zostaje
38, MSE na G, stabilny trening dominacja D, zostaje
39, Nieznaczne rozszerzenie D [NOISE_SIZE, 4 -> 8, 16, 128, GENERATOR_SAMPLES_TO_RETURN], G dominuje, powrot
40, Rozszerzenie D 4 -> 6, obydwa sie ucza, na dluga mete G dominuje, puscmy go tak na dluzej, zostaje
41, LR_RATIO 15 -> 17 liczba epok 5k -> 15k, D dominuje, powrot
42, LR_RATIO 15 -> 12, nadal D dominuje, powrot
43, LR_RATIO 15 -> 5, uczy sie w miare ok, zostaje
44, LR_RATIO 5 -> 2.5, poki co lokalne minimum, zostaje
45, LR_RATIO 2.5 -> 1.25, wciaz D dominuje, zostaje 
46, nieco rozszerzyc G [NOISE_SIZE, 6 -> 256, 16 -> 256, 128, GENERATOR_SAMPLES_TO_RETURN], Caly czas dominuje D, zostaje 
47, LR_RATIO 1 -> 0.5, szybciej lepiej zbiega wciaz D dominuje, zostaje
48, ReLU na G, D calkowicie dominuje bardzo dziwny rozklad G, powrot,
49, LR_CONSTANT 7.5e-4 -> 1e-3 LR_RATIO 0.5 -> 0.25, podobnie jak z ReLU
50, G optim RMS -> Adam, wciaz dominacja D, zostaje
51, zmiana D [GENERATOR_SAMPLES_TO_RETURN, 512, 256, 128, 64, 32, 16, 8, 4, 2, 1] -> [GENERATOR_SAMPLES_TO_RETURN, 256, 64, 32, 16, 8, 4, 2, 1], jeszcze szybsza dominacja D, zostaje
52, GENERATOR_SAMPLES_TO_RETURN 512 -> 256, Po 5k epoch D dominuje dla duzych r rozklad jest ladny ale G ma problem z rozkladem kata, zostaje
53, bez BatchNorma na G, wywalilo x i y w kosmos 10^8 wybuch wag D sobie nie radzi?, zostaje
54, ReLU na G max_norm G na 3.0, x i y z G nadal ida w kosmos ale to G dominuje, zostaje
55, max_norm na G na 7.0, nadal D dominuje, zostaje
56, zdjecie max_norma na D LR_CONSTANT 1e-3 -> 2.5e-3, Nadal D dominuje, zostaje,
57, GENERATOR_SAMPLES_TO_RETURN 256 -> 512, D znaczaco dominuje, zostaje
58, NOISE_SIZE 128 -> 256, po 3000 epok dominacja dyskryminatora kompletna, powrot
59, GENERATOR_SAMPLES_TO_RETURN 512 -> 256, stabilna dominacja D, zostaje
60, G.optim ADAM -> RMSprop, G w koncu dominuje probki ida w kosmos, zostaje
61, Nowa warstwa na D 512 neuronow po GENERATOR_SAMPLES_TO_RETURN, D dominuje, powrot
62, LR_RATIO 0.25 -> 1, ewidentna dominacja D, powrot
63, GENERATOR_SAMPLES_TO_RETURN 256 -> 512, wieksza dominacja D ale wartosci srednie promienia nie ida w nieskonczonosc na koncu strata D = 1 nie do konca wiem czemu, zostaje
64, Zwezenie D warstwa 2 256n -> 128n, D dominuje ale promienie w miare normalnie sie zachowuja, zostaje
65, LR_CONSTANT 2e-3 -> 5e-4, dominacja D na poczatku wygrywal G rozklad wygladal w miare ok ale jest problem z tym ze powstaja dziwne piki / rozklad nie jest ciagli, zostaje
66, usuniecie batchNorma z G, G dominuje probki leca w kosmos od 4000 brak nauki, zostaje
67, LR_RATIO 0.25 -> 1, wybuch probek szybki koniec nauki, powrot
68, D.optim ADAM -> RMSprop, probki z G ida w kosmos szybko, powrot
69, spectral_norm na D, calkowita i szybka dominacja D, powrot
70, dwie nowe warstwy na D nowa warstwa1 512n warstwa3 256n, sprowadzenie probek z G na ziemie ale G calkowicie dominuje wciaz wartosci wysokie wysokie KL, zostaje 
71, przywrocenie batchNorma w G, D znaczaco dominuje od 2500 epoki probki sa sprowadzane na ziemie od 4000 epoki brak uczenia, zostaje
72, NOISE_SIZE 128 -> 512, chwilowo D dominuje potem zaczyna dominowac G ale przez to zaczyna wyrzucac probki do nieskonczonosci niby sie one ucza ale po rozkladzie tego nie widac, powrot
73, NOISE_SIZE 128 -> 256, wydaja sie obydwie dosc stabilne G dominuje od epoki 6000, zostaje
74, Wyrzucenie ReLU z G, Wynik fajny wyniki sa mniej "Zkwantowane" bardziej ciagle D dominuje od 2000 epoki, zostaje
75, Rozszerzenie G z [NOISE_SIZE, 256, 256, 256, 256, 256, GENERATOR_SAMPLES_TO_RETURN] na [NOISE_SIZE, 512, 256, 128, 256, 512, GENERATOR_SAMPLES_TO_RETURN], od 2000 D dominuje, zostaje
76, Warstwe 3 na G z 128 -> 256 neuronow dodam jeszcze jedna warstwe, mniej wiecej jest rownowaga ze wskazaniem na G, zostaje
77, LR_RATIO 0.25 -> 1, Troche za duzo D stale wygrywa czasem dominuje, powrot
78, LR_RATIO 0.25 -> 0.5, stabilniej, zostaje
79, GENERATOR_SAMPLES_TO_RETURN 512 -> 768, W miare stabilnie do momentu kiedy D nie zaczyna dominowac, zostaje
80, LR_RATIO 0.5 -> 0.4, D szybciej dominuje od 2000 epoki do 9000 epoki KL szybko rosnie, powrot
81, LR_RATIO 0.5 -> 0.6, uczy sie ale ciezko przewidziec skutki jest w miare ok, zostaje
82, LR_RATIO 0.6 -> 0.7, Wszystko poza tym ze x^2 i y^2 nie za bardzo dostosowuja sie do rozkladu. 
83, Dodanie ponalizacji na G 0.1 * MSE(E[x_true^2] - E[x_fake^2]), strata sztucznie G powiekszona nie powinien tego wyswietlac D dominuje od 2000 epoki, zostaje
84, LR_RATIO 0.7 -> 1, Stabilnie sie uczy ale nie wyglada jakby mial sie dopasowac do piku, zostaje
85, Zwiekszyc stala penalizacji na G do 0.3, srednie r na G wystrzelilo do 14 wygral D od 3000 epoki D zdominowal, zostaje
86, Stala penalizacji na 1, dokladnie to samo jeszcze przed 3000 epokao totalnie to nie dziala zgaduje ze pojedyncze probki daleko od centrum z rozkladu prawdziwego pokazuja czy jest ona caly rozklad prawdziwy czy nie, powrot
87, zamiast sredniej kwadratow to odchyelnie std jako penalizator, wykresy nie pokazuja aby cokolwiek z odch. sie poprawilo w G, zostaje 
